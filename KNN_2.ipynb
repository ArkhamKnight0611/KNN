{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Euclidean vs. Manhattan Distance in KNN\n",
        "\n",
        "Euclidean Distance: This metric calculates the straight-line distance between two data points in n-dimensional space. It considers the magnitude of differences along all features.\n",
        "\n",
        "Manhattan Distance (City Block Distance): This metric sums the absolute differences between corresponding coordinates of two data points. It imagines traveling along a grid, taking the sum of the horizontal and vertical distances between points.\n",
        "\n",
        "Impact on Performance:\n",
        "\n",
        "Euclidean Distance: More sensitive to features with larger scales. If features aren't scaled, those with larger scales will dominate distance calculations, potentially leading to misleading nearest neighbors, especially in high-dimensional data.\n",
        "Manhattan Distance: Less sensitive to outliers or extreme values in single features. This can be beneficial for datasets with outliers that might skew Euclidean distances.\n",
        "Choosing a Distance Metric:\n",
        "\n",
        "Use Euclidean distance as the default for most cases.\n",
        "Consider Manhattan distance if your data has outliers or features with significant scale differences you don't want to emphasize.\n",
        "Q2. Finding the Optimal K Value\n",
        "\n",
        "There's no single \"best\" value for K, but here's how to find an optimal K for your KNN model:\n",
        "\n",
        "Experimentation: Train your KNN model with different K values and evaluate its performance on a held-out validation set using metrics like accuracy (classification) or MSE (regression). Choose the K that yields the best performance.\n",
        "Cross-validation: Divide your data into folds. Train the model on a subset of folds (e.g., 80%) with different K values, and evaluate on the remaining fold (e.g., 20%). Repeat this process for all folds and choose the K with the best average performance.\n",
        "Q3. Distance Metric and KNN Performance\n",
        "\n",
        "The distance metric significantly impacts KNN performance. Here's how to decide:\n",
        "\n",
        "Impact of Metric:\n",
        "Euclidean distance: Works well for smooth, spherical data distributions.\n",
        "Manhattan distance: More robust to outliers and skewed distributions.\n",
        "Choosing a Metric:\n",
        "Start with Euclidean distance as the default.\n",
        "If your data has outliers or features with vastly different scales, consider Manhattan distance.\n",
        "Evaluate performance with both metrics to see which yields better results for your specific data.\n",
        "Q4. Hyperparameter Tuning in KNN\n",
        "\n",
        "K: As discussed earlier, this is a crucial hyperparameter affecting how many neighbors influence the prediction.\n",
        "Distance Metric: The choice of distance metric (Euclidean vs. Manhattan) can significantly impact performance.\n",
        "Feature Scaling: Standardizing features ensures all contribute equally to distance calculations, leading to better neighbor selection.\n",
        "Tuning Techniques:\n",
        "\n",
        "Grid Search or Randomized Search: Evaluate different combinations of hyperparameter values and choose the one that minimizes your error metric.\n",
        "Cross-validation: As mentioned earlier, use cross-validation to assess performance for different hyperparameter configurations.\n",
        "Q5. Training Set Size and KNN\n",
        "\n",
        "Larger Training Sets: Generally, KNN models benefit from larger training sets. More data points provide a richer pool for finding true nearest neighbors, leading to potentially better predictions.\n",
        "Optimizing Training Set Size: There's a trade-off between size and computational cost. While larger datasets are preferable, extremely large ones can make training and prediction computationally expensive. Consider techniques like sampling or dimensionality reduction for very large datasets.\n",
        "Q6. Drawbacks of KNN and Addressing Them\n",
        "\n",
        "Curse of Dimensionality: Performance degrades in high dimensions due to meaningless distances between points.\n",
        "Computational Cost: Finding nearest neighbors can be computationally expensive for large datasets.\n",
        "Sensitivity to Noise: Outliers can significantly impact KNN predictions.\n",
        "Overcoming Drawbacks:\n",
        "\n",
        "Curse of Dimensionality: Use feature selection or dimensionality reduction techniques to reduce irrelevant features.\n",
        "Computational Cost: Utilize efficient data structures like Kd-trees for faster neighbor search.\n",
        "Noise Sensitivity: Consider pre-processing techniques like outlier removal or robust distance metrics less susceptible to outliers."
      ],
      "metadata": {
        "id": "oOLdGY1qjKrh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "smrV9CEvjWkX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}