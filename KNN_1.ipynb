{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the KNN algorithm?\n",
        "\n",
        "The K-Nearest Neighbors (KNN) algorithm is a fundamental supervised machine learning method used for both classification and regression tasks. It works by classifying a new data point based on the labels of its closest neighbors in the training data. In simpler terms, it predicts that a new data point will belong to the same class as the majority of its closest neighbors in the training set.\n",
        "\n",
        "Q2. How do you choose the value of K in KNN?\n",
        "\n",
        "The value of K in KNN represents the number of nearest neighbors to consider for classification or prediction. Choosing the optimal K is crucial for the algorithm's performance. Here's a guideline:\n",
        "\n",
        "Lower K: Higher variance, lower bias (more sensitive to noise)\n",
        "Higher K: Lower variance, higher bias (might underfit the data)\n",
        "Experiment with different K values and evaluate using cross-validation to find the K that minimizes your error metric.\n",
        "\n",
        "Q3. What is the difference between KNN classifier and KNN regressor?\n",
        "\n",
        "KNN Classifier: Used for classification problems where the output variable belongs to a discrete set of categories. The KNN classifier predicts the class label (e.g., spam/not spam) for a new data point based on the majority class of its K nearest neighbors.\n",
        "KNN Regressor: Used for regression problems where the output variable is continuous. The KNN regressor predicts a continuous value (e.g., house price) for a new data point by averaging the values of its K nearest neighbors, potentially weighted by their distance.\n",
        "Q4. How do you measure the performance of KNN?\n",
        "\n",
        "Common metrics for evaluating KNN performance include:\n",
        "\n",
        "Classification: Accuracy, precision, recall, F1-score\n",
        "Regression: Mean Squared Error (MSE), Root Mean Squared Error (RMSE)\n",
        "The choice of metric depends on your specific problem and what aspect of performance is most important.\n",
        "\n",
        "Q5. What is the curse of dimensionality in KNN?\n",
        "\n",
        "The curse of dimensionality refers to the phenomenon where the performance of KNN can deteriorate as the number of features (dimensions) in your data increases. Distances between data points become less meaningful in high dimensions, making it harder to find good neighbors.\n",
        "\n",
        "Q6. How do you handle missing values in KNN?\n",
        "\n",
        "Missing values can negatively impact the distance calculations in KNN. Here are some approaches:\n",
        "\n",
        "Remove data points with a high number of missing values.\n",
        "Impute missing values using techniques like mean/median imputation or more sophisticated methods.\n",
        "Q7. KNN classifier vs. regressor: Performance and suitability\n",
        "\n",
        "Performance: Generally, KNN classifiers are simpler to implement and can be effective for various classification tasks. KNN regressors can be computationally expensive, and their performance might not be as competitive as other regression algorithms in high dimensions.\n",
        "Suitability: KNN classifiers are well-suited for problems with well-defined clusters and when the class distribution is relatively balanced. KNN regressors are useful for continuous prediction tasks where the relationship between features and target variable is not well-understood.\n",
        "Q8. Strengths and weaknesses of KNN\n",
        "\n",
        "Strengths:\n",
        "Simple to understand and implement.\n",
        "No assumptions about data distribution.\n",
        "Can handle both numerical and categorical data.\n",
        "Weaknesses:\n",
        "Computationally expensive for large datasets.\n",
        "Sensitive to the choice of K and distance metric.\n",
        "Prone to the curse of dimensionality.\n",
        "Addressing weaknesses:\n",
        "\n",
        "Computational cost: Utilize techniques like Kd-trees for efficient neighbor search.\n",
        "K and distance metric: Experiment with different values and metrics to find the optimal configuration.\n",
        "Curse of dimensionality: Feature selection or dimensionality reduction techniques can help.\n",
        "Q9. Euclidean vs. Manhattan distance in KNN\n",
        "\n",
        "Euclidean distance: Straight-line distance between two data points in n-dimensional space.\n",
        "Manhattan distance: Sum of the absolute differences between corresponding coordinates of two data points.\n",
        "The choice of distance metric depends on your data and the underlying relationships between features. Euclidean distance is commonly used, but Manhattan distance can be more robust to outliers in certain scenarios.\n",
        "\n",
        "Q10. Role of feature scaling in KNN\n",
        "\n",
        "Feature scaling ensures that all features contribute equally to the distance calculations in KNN. Features with larger scales will dominate the distance if not scaled, potentially leading to misleading neighbor selections. Standardizing or normalizing features to a common range is recommended for optimal KNN performance."
      ],
      "metadata": {
        "id": "fT_WwmHIe8HC"
      }
    }
  ]
}